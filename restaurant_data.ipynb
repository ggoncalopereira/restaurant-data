{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e18019d8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-27T18:06:37.417137Z",
     "iopub.status.busy": "2024-08-27T18:06:37.416692Z",
     "iopub.status.idle": "2024-08-27T18:07:37.996580Z",
     "shell.execute_reply": "2024-08-27T18:07:37.995280Z"
    },
    "papermill": {
     "duration": 60.590391,
     "end_time": "2024-08-27T18:07:37.999569",
     "exception": false,
     "start_time": "2024-08-27T18:06:37.409178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\r\n",
      "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\r\n",
      "Building wheels for collected packages: pyspark\r\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812364 sha256=4b0f4f9738bb86e2145d9dd67f9cdf20c728445c8e8a0d0d533fd5f19614912f\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\r\n",
      "Successfully built pyspark\r\n",
      "Installing collected packages: pyspark\r\n",
      "Successfully installed pyspark-3.5.2\r\n"
     ]
    }
   ],
   "source": [
    "# Install pyspark to begin with\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea686c7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T18:07:38.054699Z",
     "iopub.status.busy": "2024-08-27T18:07:38.054226Z",
     "iopub.status.idle": "2024-08-27T18:07:43.821700Z",
     "shell.execute_reply": "2024-08-27T18:07:43.819714Z"
    },
    "papermill": {
     "duration": 5.797961,
     "end_time": "2024-08-27T18:07:43.824746",
     "exception": false,
     "start_time": "2024-08-27T18:07:38.026785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/27 18:07:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "                    .appName(\"Restaurants\")\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b79adc4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T18:07:43.892436Z",
     "iopub.status.busy": "2024-08-27T18:07:43.890986Z",
     "iopub.status.idle": "2024-08-27T18:07:53.417331Z",
     "shell.execute_reply": "2024-08-27T18:07:53.416211Z"
    },
    "papermill": {
     "duration": 9.570321,
     "end_time": "2024-08-27T18:07:53.421525",
     "exception": false,
     "start_time": "2024-08-27T18:07:43.851204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------+-----+\n",
      "|               name1|            address1|           city1| zip1|\n",
      "+--------------------+--------------------+----------------+-----+\n",
      "|BENTONS ABSOLUTE ...|    23820 ROCKINGHAM|      SOUTHFIELD|48034|\n",
      "|BENTONS ABSOLUTE ...|    23820 ROCKINGHAM|      SOUTHFIELD|48034|\n",
      "|    PRO WINDMILL INC|   178 SPRING STREET|         YANKTON|57078|\n",
      "|OLSONS PEST TECHN...|   178 SPRING STREET|         YANKTON|57078|\n",
      "|INTERIM HEALTH CA...|1901 NORTH UNION ...|COLORADO SPRINGS|80909|\n",
      "| INTERIM HEALTH CARE|1901 NORTH UNION ...|COLORADO SPRINGS|80909|\n",
      "|   COUNTY OF PASSAIC|     40 VALLEY ZROAD|         HALEDON| 7508|\n",
      "|PREAKNESS HEALTH ...|     40 VALLEY ZROAD|         HALEDON| 7508|\n",
      "|RESIDENT MANAGEME...|1390 QUAIL LAKE LOOP|COLORADO SPRINGS|80906|\n",
      "|RESIDENT MANAGEME...|1390 QUAIL LAKE LOOP|COLORADO SPRINGS|80906|\n",
      "|FENIMORE DRYWALL INC|    5609 NEWLAND WAY|          ARVADA|80002|\n",
      "|SAM KEDEM NURSERY...|12414 191ST STREE...|        HASTINGS|55033|\n",
      "|   SAM KEDEM NURSERY|12414 191ST STREE...|        HASTINGS|55033|\n",
      "|     OC CLEANERS INC|   304 EAST MICHIGAN|       SPEARFISH|57783|\n",
      "|        O C CLEANERS|   304 EAST MICHIGAN|       SPEARFISH|57783|\n",
      "|BRASS KEY PROPERT...|115 RIVERSIDE AVENUE|    FORT COLLINS|80524|\n",
      "|BRASS KEY PROPERT...|115 RIVERSIDE AVENUE|    FORT COLLINS|80524|\n",
      "|     KENDALL L KLAUS|   17759 KIRBY AVE S|        HASTINGS|55033|\n",
      "|     KLAUS NURSERIES|   17759 KIRBY AVE S|        HASTINGS|55033|\n",
      "|KINSALE PROPERTIE...|    1417 BARLOW ROAD|     FORT MORGAN|80701|\n",
      "+--------------------+--------------------+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- name1: string (nullable = true)\n",
      " |-- address1: string (nullable = true)\n",
      " |-- city1: string (nullable = true)\n",
      " |-- zip1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe for the first file\n",
    "df1 = spark.read\\\n",
    "           .format(\"csv\")\\\n",
    "           .option(\"header\", True)\\\n",
    "           .option(\"inferSchema\", True)\\\n",
    "           .load(\"/kaggle/input/restaurants/file1.csv\")\n",
    "\n",
    "df1.show()\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2219827",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T18:07:53.501868Z",
     "iopub.status.busy": "2024-08-27T18:07:53.501289Z",
     "iopub.status.idle": "2024-08-27T18:07:54.430542Z",
     "shell.execute_reply": "2024-08-27T18:07:54.429462Z"
    },
    "papermill": {
     "duration": 0.973318,
     "end_time": "2024-08-27T18:07:54.433625",
     "exception": false,
     "start_time": "2024-08-27T18:07:53.460307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------+-----+\n",
      "|               name2|            address2|            city2| zip2|\n",
      "+--------------------+--------------------+-----------------+-----+\n",
      "|CARIBE TELENO RES...|      320 WATERS AVE|            TAMPA|33604|\n",
      "|LATIN MIX RESTAUR...|     11368 SW 184 ST|            MIAMI|33157|\n",
      "|FRIENDLY & AMIGO ...|12275 COLLIER BLVD 3|           NAPLES|34116|\n",
      "|       MARKER 85 LLC|    127 BAYSHORE WAY|         GOODLAND|34140|\n",
      "|         T MAX GROUP|      3743 PAULA AVE|         KEY WEST|33040|\n",
      "|PEI WEI FRESH KIT...|9982 GLADES RD ST...|       BOCA RATON|33434|\n",
      "|JULIAS PIZZA REST...|   5075 EDGEWATER DR|          ORLANDO|32810|\n",
      "|      PITAS REPUBLIC|10454 ROOSEVELT B...|   ST. PETERSBURG|33716|\n",
      "|THE HABIT BURGER ...|1801 S FEDERAL HW...|     DELRAY BEACH|33483|\n",
      "|      CRYSTAL BUFFET|3160 W NEW HAVEN AVE|      W MELBOURNE|32904|\n",
      "| MARITZA TRETO PEREZ|         368 W 14 ST|          HIALEAH|33010|\n",
      "|        SOUP TO NUTS|      1323 S MAIN ST|      GAINESVILLE|32608|\n",
      "|       SHRIMP BASKET|12390 FRONT BEACH RD|PANAMA CITY BEACH|32407|\n",
      "|CUBAN COFFEE QUEE...|   5 KEY LIME SQUARE|         KEY WEST|33040|\n",
      "|          YOGEN FRUZ|5250 INTERNATIONA...|          ORLANDO|32819|\n",
      "|         SUBWAY 1960|8000 W BROWARD BL...|       PLANTATION|33388|\n",
      "| WYNNS HEALTHY OASIS|        5450 YMCA RD|           NAPLES|34109|\n",
      "|PAPA MURPHYS TAKE...|2200 WINTER SPRIN...|           OVIEDO|32765|\n",
      "|    LAS PIEDRAS CAFE|      330 E 9 ST 106|          HIALEAH|33010|\n",
      "|A TASTE OF LOVE CAFE|     2957 EDISON AVE|     JACKSONVILLE|32254|\n",
      "+--------------------+--------------------+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- name2: string (nullable = true)\n",
      " |-- address2: string (nullable = true)\n",
      " |-- city2: string (nullable = true)\n",
      " |-- zip2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe for the second file\n",
    "df2 = spark.read\\\n",
    "           .format(\"csv\")\\\n",
    "           .option(\"header\", True)\\\n",
    "           .option(\"inferSchema\", True)\\\n",
    "           .load(\"/kaggle/input/restaurants/file2.csv\")\n",
    "\n",
    "df2.show()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9b92182",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T18:07:54.489246Z",
     "iopub.status.busy": "2024-08-27T18:07:54.488816Z",
     "iopub.status.idle": "2024-08-27T18:07:54.522255Z",
     "shell.execute_reply": "2024-08-27T18:07:54.520953Z"
    },
    "papermill": {
     "duration": 0.065189,
     "end_time": "2024-08-27T18:07:54.525731",
     "exception": false,
     "start_time": "2024-08-27T18:07:54.460542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Harmonize column names between datasets\n",
    "df1 = df1.withColumnsRenamed({'name1' : 'name', 'address1' : 'address', 'city1' : 'city', 'zip1' : 'zip'})\n",
    "df2 = df2.withColumnsRenamed({'name2' : 'name', 'address2' : 'address', 'city2' : 'city', 'zip2' : 'zip'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82ed8cb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T18:07:54.607150Z",
     "iopub.status.busy": "2024-08-27T18:07:54.605748Z",
     "iopub.status.idle": "2024-08-27T18:07:54.657869Z",
     "shell.execute_reply": "2024-08-27T18:07:54.656672Z"
    },
    "papermill": {
     "duration": 0.096226,
     "end_time": "2024-08-27T18:07:54.661049",
     "exception": false,
     "start_time": "2024-08-27T18:07:54.564823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add origin file name to each dataset\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df1 = df1.withColumn(\"origin\", lit(\"file1\"))\n",
    "df2 = df2.withColumn(\"origin\", lit(\"file2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "907fb1e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T18:07:54.739721Z",
     "iopub.status.busy": "2024-08-27T18:07:54.738797Z",
     "iopub.status.idle": "2024-08-27T18:07:54.745364Z",
     "shell.execute_reply": "2024-08-27T18:07:54.744326Z"
    },
    "papermill": {
     "duration": 0.049598,
     "end_time": "2024-08-27T18:07:54.748187",
     "exception": false,
     "start_time": "2024-08-27T18:07:54.698589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data needs to be standardized:\n",
    "# - Uniformize the characters (so that we don't get differences in uppercased/lowercased characters)\n",
    "# - Remove any non-alphanumeric character by:\n",
    "# --- Removing any non-word character (a-zA-Z0-9 or \\w)\n",
    "# --- Removing any non-whitespace character (\\n\\t + the rest for compatibility purposes?) (also works with \\s)\n",
    "from pyspark.sql.functions import upper, regexp_replace\n",
    "\n",
    "def clean_data(column):\n",
    "        return upper(regexp_replace(column, r'[^\\w\\s]', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e93bf22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T18:07:54.826793Z",
     "iopub.status.busy": "2024-08-27T18:07:54.825851Z",
     "iopub.status.idle": "2024-08-27T18:07:55.048847Z",
     "shell.execute_reply": "2024-08-27T18:07:55.047664Z"
    },
    "papermill": {
     "duration": 0.266386,
     "end_time": "2024-08-27T18:07:55.052529",
     "exception": false,
     "start_time": "2024-08-27T18:07:54.786143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply the previous cell's function to each column\n",
    "from pyspark.sql import functions as F\n",
    "# Kaggle importing pyspark doesn't allow (?) for col to be called directly\n",
    "\n",
    "df1_cleaned = df1.withColumn('name', clean_data(F.col('name')))\\\n",
    "                 .withColumn('address', clean_data(F.col('address')))\\\n",
    "                 .withColumn('city', clean_data(F.col('city')))\\\n",
    "                 .withColumn('zip', clean_data(F.col('zip')))\n",
    "\n",
    "df2_cleaned = df2.withColumn('name', clean_data(F.col('name')))\\\n",
    "                 .withColumn('address', clean_data(F.col('address')))\\\n",
    "                 .withColumn('city', clean_data(F.col('city')))\\\n",
    "                 .withColumn('zip', clean_data(F.col('zip')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cb57895",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T18:07:55.112122Z",
     "iopub.status.busy": "2024-08-27T18:07:55.111686Z",
     "iopub.status.idle": "2024-08-27T18:07:55.194399Z",
     "shell.execute_reply": "2024-08-27T18:07:55.193134Z"
    },
    "papermill": {
     "duration": 0.114584,
     "end_time": "2024-08-27T18:07:55.198056",
     "exception": false,
     "start_time": "2024-08-27T18:07:55.083472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data now has origin file and hashed fields\n",
    "# We can safely merge the two files so we can work on the output\n",
    "\n",
    "merged_df = df1_cleaned.unionAll(df2_cleaned)\n",
    "\n",
    "# Should we drop the NAN/nulls? If so, \n",
    "merged_df = merged_df.dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ede98de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T18:07:55.279212Z",
     "iopub.status.busy": "2024-08-27T18:07:55.278664Z",
     "iopub.status.idle": "2024-08-27T18:07:55.411759Z",
     "shell.execute_reply": "2024-08-27T18:07:55.410631Z"
    },
    "papermill": {
     "duration": 0.177685,
     "end_time": "2024-08-27T18:07:55.415115",
     "exception": false,
     "start_time": "2024-08-27T18:07:55.237430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We need to:\n",
    "# - Output all restaurants and addresses without duplicates\n",
    "# - Output the hashed field\n",
    "# - Output the total number of occurrences of each entry (group the entries)\n",
    "# - Output two boolean flags that are true/false each depending on the values of \"origin\"\n",
    "from pyspark.sql.functions import count, when\n",
    "\n",
    "########\n",
    "# Group the dataframe by the relevant columns to filter out duplicates\n",
    "# By doing so, we will also aggregate the grouped results so as to add new columns.\n",
    "########\n",
    "# First count will just count the # of occurrences of \"same values\".\n",
    "# Whereas the second and third counts will look into the existence of \"file1\" or \"file2\" on the \"origin\" column,\n",
    "# and passing a truthful value to a new column depending on whether it exists or not\n",
    "########\n",
    "\n",
    "final_df = merged_df.groupBy('name', 'address', 'city', 'zip')\\\n",
    "                    .agg(\n",
    "                        count(\"*\").alias('number_of_occurrences'),\n",
    "                        (count(when(F.col('origin') == 'file1', True)) > 0).alias('exists_file1'),\n",
    "                        (count(when(F.col('origin') == 'file2', True)) > 0).alias('exists_file2')\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e6ead2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T18:07:55.492943Z",
     "iopub.status.busy": "2024-08-27T18:07:55.491996Z",
     "iopub.status.idle": "2024-08-27T18:07:55.570896Z",
     "shell.execute_reply": "2024-08-27T18:07:55.569415Z"
    },
    "papermill": {
     "duration": 0.121936,
     "end_time": "2024-08-27T18:07:55.574673",
     "exception": false,
     "start_time": "2024-08-27T18:07:55.452737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add unique identifier for each cleaned file\n",
    "# First concat all the relevant fields\n",
    "# Then hash over the concatenated field\n",
    "from pyspark.sql.functions import concat_ws, sha2\n",
    "\n",
    "# Concat all relevant columns with a '_'\n",
    "final_df = final_df.withColumn('uuid', concat_ws(\"_\", F.col('name'), F.col('address'), F.col('city'), F.col('zip')))\n",
    "\n",
    "# Apply sha256 over the concatenated string\n",
    "final_df = final_df.withColumn('uuid', sha2(F.col('uuid'), 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "225e0efa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T18:07:55.649216Z",
     "iopub.status.busy": "2024-08-27T18:07:55.648121Z",
     "iopub.status.idle": "2024-08-27T18:08:04.619833Z",
     "shell.execute_reply": "2024-08-27T18:08:04.618754Z"
    },
    "papermill": {
     "duration": 9.011785,
     "end_time": "2024-08-27T18:08:04.625406",
     "exception": false,
     "start_time": "2024-08-27T18:07:55.613621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------+-----+------------+------------+---------------------+\n",
      "|                uuid|                name|             address|         city|  zip|exists_file1|exists_file2|number_of_occurrences|\n",
      "+--------------------+--------------------+--------------------+-------------+-----+------------+------------+---------------------+\n",
      "|66e4647885fec9266...|   US POSTAL SERVICE|     160 DURYEA ROAD|     MELVILLE|11747|        true|       false|                   49|\n",
      "|ad429e3e35a455879...|X PRESS SWEEPING INC|        6 CRUDALE DR| WEST WARWICK| 2893|        true|       false|                   46|\n",
      "|2ce0a86068e5fa5bf...|USPROTECT CORPORA...|801 ROEDER ROAD S...|SILVER SPRING|20910|        true|       false|                   43|\n",
      "|fbdd5fda1155bed4d...|USPS MID ISLAND P...|     160 DURYEA ROAD|     MELVILLE|11747|        true|       false|                   28|\n",
      "|af91c0b850e98204d...|                USPS|     160 DURYEA ROAD|     MELVILLE|11747|        true|       false|                   26|\n",
      "|154faa5fc4b3acfb7...|      JD PRODUCE INC|         PO BOX 1548|     EDINBURG|78539|        true|       false|                   22|\n",
      "|de5c6d0c25c8354fe...|FRONTERA PRODUCE LTD|         PO BOX 2019|     EDINBURG|78540|        true|       false|                   21|\n",
      "|9e9be3847cee3736c...|       SLM TRANS INC|8401 CORPORATE DR...|  HYATTSVILLE|20785|        true|       false|                   20|\n",
      "|69bb95f0bc75fb593...|       RIO FRESH INC|         PO BOX 1619|     SAN JUAN|78589|        true|       false|                   20|\n",
      "|d159f7122f1413ede...|          US PROTECT|801 ROEDER ROAD S...|SILVER SPRING|20910|        true|       false|                   20|\n",
      "+--------------------+--------------------+--------------------+-------------+-----+------------+------------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- uuid: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- zip: string (nullable = true)\n",
      " |-- exists_file1: boolean (nullable = false)\n",
      " |-- exists_file2: boolean (nullable = false)\n",
      " |-- number_of_occurrences: long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Make the ordering a bit nicer...\n",
    "\n",
    "final_df = final_df.select('uuid', 'name', 'address', 'city', 'zip', 'exists_file1', 'exists_file2', 'number_of_occurrences').sort('number_of_occurrences', ascending=False)\n",
    "\n",
    "final_df.show(10)\n",
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1319f800",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T18:08:04.682469Z",
     "iopub.status.busy": "2024-08-27T18:08:04.681437Z",
     "iopub.status.idle": "2024-08-27T18:08:14.303853Z",
     "shell.execute_reply": "2024-08-27T18:08:14.302706Z"
    },
    "papermill": {
     "duration": 9.653405,
     "end_time": "2024-08-27T18:08:14.306401",
     "exception": false,
     "start_time": "2024-08-27T18:08:04.652996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Finally, write the output to a csv file\n",
    "\n",
    "final_df.write.csv(\"output.csv\", header=True, mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5587675,
     "sourceId": 9237680,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 102.501836,
   "end_time": "2024-08-27T18:08:16.958136",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-27T18:06:34.456300",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
