{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9237680,"sourceType":"datasetVersion","datasetId":5587675}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install pyspark to begin with\n!pip install pyspark","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-27T17:19:05.741374Z","iopub.execute_input":"2024-08-27T17:19:05.742801Z","iopub.status.idle":"2024-08-27T17:19:21.835691Z","shell.execute_reply.started":"2024-08-27T17:19:05.742713Z","shell.execute_reply":"2024-08-27T17:19:21.833827Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyspark in /opt/conda/lib/python3.10/site-packages (3.5.2)\nRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql import SparkSession\n\n# Create SparkSession\n\nspark = SparkSession.builder\\\n                    .appName(\"Restaurants\")\\\n                    .getOrCreate()","metadata":{"execution":{"iopub.status.busy":"2024-08-27T17:19:21.839467Z","iopub.execute_input":"2024-08-27T17:19:21.840166Z","iopub.status.idle":"2024-08-27T17:19:21.857147Z","shell.execute_reply.started":"2024-08-27T17:19:21.840097Z","shell.execute_reply":"2024-08-27T17:19:21.855442Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Create dataframe for the first file\ndf1 = spark.read\\\n           .format(\"csv\")\\\n           .option(\"header\", True)\\\n           .option(\"inferSchema\", True)\\\n           .load(\"/kaggle/input/restaurants/file1.csv\")\n\ndf1.show()\ndf1.printSchema()","metadata":{"execution":{"iopub.status.busy":"2024-08-27T17:19:21.859509Z","iopub.execute_input":"2024-08-27T17:19:21.860109Z","iopub.status.idle":"2024-08-27T17:19:23.112225Z","shell.execute_reply.started":"2024-08-27T17:19:21.860053Z","shell.execute_reply":"2024-08-27T17:19:23.111097Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"[Stage 15:==============>                                           (1 + 3) / 4]\r","output_type":"stream"},{"name":"stdout","text":"+--------------------+--------------------+----------------+-----+\n|               name1|            address1|           city1| zip1|\n+--------------------+--------------------+----------------+-----+\n|BENTONS ABSOLUTE ...|    23820 ROCKINGHAM|      SOUTHFIELD|48034|\n|BENTONS ABSOLUTE ...|    23820 ROCKINGHAM|      SOUTHFIELD|48034|\n|    PRO WINDMILL INC|   178 SPRING STREET|         YANKTON|57078|\n|OLSONS PEST TECHN...|   178 SPRING STREET|         YANKTON|57078|\n|INTERIM HEALTH CA...|1901 NORTH UNION ...|COLORADO SPRINGS|80909|\n| INTERIM HEALTH CARE|1901 NORTH UNION ...|COLORADO SPRINGS|80909|\n|   COUNTY OF PASSAIC|     40 VALLEY ZROAD|         HALEDON| 7508|\n|PREAKNESS HEALTH ...|     40 VALLEY ZROAD|         HALEDON| 7508|\n|RESIDENT MANAGEME...|1390 QUAIL LAKE LOOP|COLORADO SPRINGS|80906|\n|RESIDENT MANAGEME...|1390 QUAIL LAKE LOOP|COLORADO SPRINGS|80906|\n|FENIMORE DRYWALL INC|    5609 NEWLAND WAY|          ARVADA|80002|\n|SAM KEDEM NURSERY...|12414 191ST STREE...|        HASTINGS|55033|\n|   SAM KEDEM NURSERY|12414 191ST STREE...|        HASTINGS|55033|\n|     OC CLEANERS INC|   304 EAST MICHIGAN|       SPEARFISH|57783|\n|        O C CLEANERS|   304 EAST MICHIGAN|       SPEARFISH|57783|\n|BRASS KEY PROPERT...|115 RIVERSIDE AVENUE|    FORT COLLINS|80524|\n|BRASS KEY PROPERT...|115 RIVERSIDE AVENUE|    FORT COLLINS|80524|\n|     KENDALL L KLAUS|   17759 KIRBY AVE S|        HASTINGS|55033|\n|     KLAUS NURSERIES|   17759 KIRBY AVE S|        HASTINGS|55033|\n|KINSALE PROPERTIE...|    1417 BARLOW ROAD|     FORT MORGAN|80701|\n+--------------------+--------------------+----------------+-----+\nonly showing top 20 rows\n\nroot\n |-- name1: string (nullable = true)\n |-- address1: string (nullable = true)\n |-- city1: string (nullable = true)\n |-- zip1: string (nullable = true)\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"# Create dataframe for the second file\ndf2 = spark.read\\\n           .format(\"csv\")\\\n           .option(\"header\", True)\\\n           .option(\"inferSchema\", True)\\\n           .load(\"/kaggle/input/restaurants/file2.csv\")\n\ndf2.show()\ndf2.printSchema()","metadata":{"execution":{"iopub.status.busy":"2024-08-27T17:19:23.115716Z","iopub.execute_input":"2024-08-27T17:19:23.116180Z","iopub.status.idle":"2024-08-27T17:19:24.029198Z","shell.execute_reply.started":"2024-08-27T17:19:23.116128Z","shell.execute_reply":"2024-08-27T17:19:24.028021Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"+--------------------+--------------------+-----------------+-----+\n|               name2|            address2|            city2| zip2|\n+--------------------+--------------------+-----------------+-----+\n|CARIBE TELENO RES...|      320 WATERS AVE|            TAMPA|33604|\n|LATIN MIX RESTAUR...|     11368 SW 184 ST|            MIAMI|33157|\n|FRIENDLY & AMIGO ...|12275 COLLIER BLVD 3|           NAPLES|34116|\n|       MARKER 85 LLC|    127 BAYSHORE WAY|         GOODLAND|34140|\n|         T MAX GROUP|      3743 PAULA AVE|         KEY WEST|33040|\n|PEI WEI FRESH KIT...|9982 GLADES RD ST...|       BOCA RATON|33434|\n|JULIAS PIZZA REST...|   5075 EDGEWATER DR|          ORLANDO|32810|\n|      PITAS REPUBLIC|10454 ROOSEVELT B...|   ST. PETERSBURG|33716|\n|THE HABIT BURGER ...|1801 S FEDERAL HW...|     DELRAY BEACH|33483|\n|      CRYSTAL BUFFET|3160 W NEW HAVEN AVE|      W MELBOURNE|32904|\n| MARITZA TRETO PEREZ|         368 W 14 ST|          HIALEAH|33010|\n|        SOUP TO NUTS|      1323 S MAIN ST|      GAINESVILLE|32608|\n|       SHRIMP BASKET|12390 FRONT BEACH RD|PANAMA CITY BEACH|32407|\n|CUBAN COFFEE QUEE...|   5 KEY LIME SQUARE|         KEY WEST|33040|\n|          YOGEN FRUZ|5250 INTERNATIONA...|          ORLANDO|32819|\n|         SUBWAY 1960|8000 W BROWARD BL...|       PLANTATION|33388|\n| WYNNS HEALTHY OASIS|        5450 YMCA RD|           NAPLES|34109|\n|PAPA MURPHYS TAKE...|2200 WINTER SPRIN...|           OVIEDO|32765|\n|    LAS PIEDRAS CAFE|      330 E 9 ST 106|          HIALEAH|33010|\n|A TASTE OF LOVE CAFE|     2957 EDISON AVE|     JACKSONVILLE|32254|\n+--------------------+--------------------+-----------------+-----+\nonly showing top 20 rows\n\nroot\n |-- name2: string (nullable = true)\n |-- address2: string (nullable = true)\n |-- city2: string (nullable = true)\n |-- zip2: string (nullable = true)\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Harmonize column names between datasets\ndf1 = df1.withColumnsRenamed({'name1' : 'name', 'address1' : 'address', 'city1' : 'city', 'zip1' : 'zip'})\ndf2 = df2.withColumnsRenamed({'name2' : 'name', 'address2' : 'address', 'city2' : 'city', 'zip2' : 'zip'})","metadata":{"execution":{"iopub.status.busy":"2024-08-27T17:19:24.030602Z","iopub.execute_input":"2024-08-27T17:19:24.031074Z","iopub.status.idle":"2024-08-27T17:19:24.063438Z","shell.execute_reply.started":"2024-08-27T17:19:24.031025Z","shell.execute_reply":"2024-08-27T17:19:24.062096Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Add origin file name to each dataset\nfrom pyspark.sql.functions import lit\n\ndf1 = df1.withColumn(\"origin\", lit(\"file1\"))\ndf2 = df2.withColumn(\"origin\", lit(\"file2\"))","metadata":{"execution":{"iopub.status.busy":"2024-08-27T17:19:24.064927Z","iopub.execute_input":"2024-08-27T17:19:24.070118Z","iopub.status.idle":"2024-08-27T17:19:24.118674Z","shell.execute_reply.started":"2024-08-27T17:19:24.070049Z","shell.execute_reply":"2024-08-27T17:19:24.117465Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Data needs to be standardized:\n# - Uniformize the characters (so that we don't get differences in uppercased/lowercased characters)\n# - Remove any non-alphanumeric character by:\n# --- Removing any non-word character (a-zA-Z0-9 or \\w)\n# --- Removing any non-whitespace character (\\n\\t + the rest for compatibility purposes?) (also works with \\s)\nfrom pyspark.sql.functions import upper, regexp_replace\n\ndef clean_data(column):\n        return upper(regexp_replace(column, r'[^\\w\\s]', ''))","metadata":{"execution":{"iopub.status.busy":"2024-08-27T17:19:24.120252Z","iopub.execute_input":"2024-08-27T17:19:24.120849Z","iopub.status.idle":"2024-08-27T17:19:24.140570Z","shell.execute_reply.started":"2024-08-27T17:19:24.120791Z","shell.execute_reply":"2024-08-27T17:19:24.138957Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Apply the previous cell's function to each column\nfrom pyspark.sql import functions as F\n# Kaggle importing pyspark doesn't allow (?) for col to be called directly\n\ndf1_cleaned = df1.withColumn('name', clean_data(F.col('name')))\\\n                 .withColumn('address', clean_data(F.col('address')))\\\n                 .withColumn('city', clean_data(F.col('city')))\\\n                 .withColumn('zip', clean_data(F.col('zip')))\n\ndf2_cleaned = df2.withColumn('name', clean_data(F.col('name')))\\\n                 .withColumn('address', clean_data(F.col('address')))\\\n                 .withColumn('city', clean_data(F.col('city')))\\\n                 .withColumn('zip', clean_data(F.col('zip')))","metadata":{"execution":{"iopub.status.busy":"2024-08-27T17:19:24.145386Z","iopub.execute_input":"2024-08-27T17:19:24.146382Z","iopub.status.idle":"2024-08-27T17:19:24.363210Z","shell.execute_reply.started":"2024-08-27T17:19:24.146325Z","shell.execute_reply":"2024-08-27T17:19:24.361487Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Data now has origin file and hashed fields\n# We can safely merge the two files so we can work on the output\n\nmerged_df = df1_cleaned.unionAll(df2_cleaned)\n\n# Should we drop the NAN/nulls? If so, \nmerged_df = merged_df.dropna() ","metadata":{"execution":{"iopub.status.busy":"2024-08-27T17:19:24.365898Z","iopub.execute_input":"2024-08-27T17:19:24.366496Z","iopub.status.idle":"2024-08-27T17:19:24.409246Z","shell.execute_reply.started":"2024-08-27T17:19:24.366437Z","shell.execute_reply":"2024-08-27T17:19:24.407779Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# We need to:\n# - Output all restaurants and addresses without duplicates\n# - Output the hashed field\n# - Output the total number of occurrences of each entry (group the entries)\n# - Output two boolean flags that are true/false each depending on the values of \"origin\"\nfrom pyspark.sql.functions import count, when\n\n########\n# Group the dataframe by the relevant columns to filter out duplicates\n# By doing so, we will also aggregate the grouped results so as to add new columns.\n########\n# First count will just count the # of occurrences of \"same values\".\n# Whereas the second and third counts will look into the existence of \"file1\" or \"file2\" on the \"origin\" column,\n# and passing a truthful value to a new column depending on whether it exists or not\n########\n\nfinal_df = merged_df.groupBy('name', 'address', 'city', 'zip')\\\n                    .agg(\n                        count(\"*\").alias('number_of_occurrences'),\n                        (count(when(F.col('origin') == 'file1', True)) > 0).alias('exists_file1'),\n                        (count(when(F.col('origin') == 'file2', True)) > 0).alias('exists_file2')\n                    )","metadata":{"execution":{"iopub.status.busy":"2024-08-27T17:19:24.414412Z","iopub.execute_input":"2024-08-27T17:19:24.415590Z","iopub.status.idle":"2024-08-27T17:19:24.496206Z","shell.execute_reply.started":"2024-08-27T17:19:24.415518Z","shell.execute_reply":"2024-08-27T17:19:24.494537Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Add unique identifier for each cleaned file\n# First concat all the relevant fields\n# Then hash over the concatenated field\nfrom pyspark.sql.functions import concat_ws, sha2\n\n# Concat all relevant columns with a '_'\nfinal_df = final_df.withColumn('uuid', concat_ws(\"_\", F.col('name'), F.col('address'), F.col('city'), F.col('zip')))\n\n# Apply sha256 over the concatenated string\nfinal_df = final_df.withColumn('uuid', sha2(F.col('uuid'), 256))","metadata":{"execution":{"iopub.status.busy":"2024-08-27T17:19:24.497802Z","iopub.execute_input":"2024-08-27T17:19:24.498216Z","iopub.status.idle":"2024-08-27T17:19:24.555724Z","shell.execute_reply.started":"2024-08-27T17:19:24.498171Z","shell.execute_reply":"2024-08-27T17:19:24.553982Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Make the ordering a bit nicer...\n\nfinal_df = final_df.select('uuid', 'name', 'address', 'city', 'zip', 'exists_file1', 'exists_file2', 'number_of_occurrences').sort('number_of_occurrences', ascending=False)\n\nfinal_df.show(10)\nfinal_df.printSchema()","metadata":{"execution":{"iopub.status.busy":"2024-08-27T17:19:24.557448Z","iopub.execute_input":"2024-08-27T17:19:24.557921Z","iopub.status.idle":"2024-08-27T17:19:31.634475Z","shell.execute_reply.started":"2024-08-27T17:19:24.557877Z","shell.execute_reply":"2024-08-27T17:19:31.632920Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"[Stage 22:>                                                         (0 + 4) / 4]\r","output_type":"stream"},{"name":"stdout","text":"+--------------------+--------------------+--------------------+-------------+-----+------------+------------+---------------------+\n|                uuid|                name|             address|         city|  zip|exists_file1|exists_file2|number_of_occurrences|\n+--------------------+--------------------+--------------------+-------------+-----+------------+------------+---------------------+\n|66e4647885fec9266...|   US POSTAL SERVICE|     160 DURYEA ROAD|     MELVILLE|11747|        true|       false|                   49|\n|ad429e3e35a455879...|X PRESS SWEEPING INC|        6 CRUDALE DR| WEST WARWICK| 2893|        true|       false|                   46|\n|2ce0a86068e5fa5bf...|USPROTECT CORPORA...|801 ROEDER ROAD S...|SILVER SPRING|20910|        true|       false|                   43|\n|fbdd5fda1155bed4d...|USPS MID ISLAND P...|     160 DURYEA ROAD|     MELVILLE|11747|        true|       false|                   28|\n|af91c0b850e98204d...|                USPS|     160 DURYEA ROAD|     MELVILLE|11747|        true|       false|                   26|\n|154faa5fc4b3acfb7...|      JD PRODUCE INC|         PO BOX 1548|     EDINBURG|78539|        true|       false|                   22|\n|de5c6d0c25c8354fe...|FRONTERA PRODUCE LTD|         PO BOX 2019|     EDINBURG|78540|        true|       false|                   21|\n|9e9be3847cee3736c...|       SLM TRANS INC|8401 CORPORATE DR...|  HYATTSVILLE|20785|        true|       false|                   20|\n|69bb95f0bc75fb593...|       RIO FRESH INC|         PO BOX 1619|     SAN JUAN|78589|        true|       false|                   20|\n|d159f7122f1413ede...|          US PROTECT|801 ROEDER ROAD S...|SILVER SPRING|20910|        true|       false|                   20|\n+--------------------+--------------------+--------------------+-------------+-----+------------+------------+---------------------+\nonly showing top 10 rows\n\nroot\n |-- uuid: string (nullable = true)\n |-- name: string (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- zip: string (nullable = true)\n |-- exists_file1: boolean (nullable = false)\n |-- exists_file2: boolean (nullable = false)\n |-- number_of_occurrences: long (nullable = false)\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"# Finally, write the output to a csv file\n\nfinal_df.write.csv(\"output.csv\", header=True, mode=\"overwrite\")","metadata":{"execution":{"iopub.status.busy":"2024-08-27T17:19:31.636040Z","iopub.execute_input":"2024-08-27T17:19:31.636582Z","iopub.status.idle":"2024-08-27T17:19:38.577114Z","shell.execute_reply.started":"2024-08-27T17:19:31.636515Z","shell.execute_reply":"2024-08-27T17:19:38.575681Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"# How it is done\nIn order to both merge and harmonize the restaurant data fetched from the given .csv files, this notebook/script performs the following steps:\n1. Data Cleaning, Merging and Harmonization:\n\t- Every column is standardized by converting text to uppercase, removing any special characters (non-alphanumeric) and whitespace characters;\n    - These columns are also uniformized (removing the suffix number) in order to simplify the eventual data union;\n    - A new string column is added on each file (\"origin\"), with its contents being \"file1\" or \"file2\", depending on its original file. This is to simplify the output column generated in the next step, in order to identify whether that row comes from a specific file or from another.\n\t- Data is then merged entirely;\n\t- Rows with empty or NaN values are also discarded after unifying all data (though ideally we could follow-up later with what is described in the \"future considerations\" section, as we are losing data with this step).\n\n\n2. Output columns:\n\t- The requested output columns are generated with the entirety of the data merged, grouping the dataset by its fields;\n\t- With the grouped data, we can work on aggregating the rows to:\n\t\t- Count the # of occurrences (to fetch the duplicates);\n            - To do so, we aggregate the grouped data, now normalized, and count its duplicates\n\t\t- Check for existence of the origin file's flag (and apply a flag to indicate its presence on such).\n            - This is done by averiguating whether the field actually exists via ```when(col(\"output\") == \"file1\", True)```, then counting the no. of its occurrences. When it does exist at least once, it will output `True`\n\t- With the dataset finalized, we can now generate a UUID/hash for the first four uniformized fields, which is done via a sha256 hash.\n\n\n3. Final Output:\n\t- The merged data is then written to a CSV file.\n\n# Issues, Future Considerations and \"What I would do if given more time\"\n### Data given is very \"repetitive\" with its duplicates, some fields are very similar despite having different names\n    An alternative to deal with this issue could be via a fuzzy matching algorithm to account for less \"false positives\" when it comes to name comparisons\n### Some addresses are partly written, with the same address being written in different ways (some shortened, some in full, for instance)\n\tTo tackle this issue, there could be some better string normalization libraries to normalize the addresses in its entirety\n\tWe could also geocode addresses (with APIs like Google Maps or others as such) to minimize duplicates\n### One column is generated per file in order to identify its origin, on the final dataset\n    This could be problematic in the way that, if we have 10000 input files, we'd have 10000 additional columns (making it cumbersome to manage the final dataset)\n        \n    Alternatively, we could either have an array, appending each occurrence to itself\n        __or__\n    We could still keep a string field and just concatenate each file to itself (making it \"File1.csv + File2.csv + ... + File10000.csv\" if it was present on every file)","metadata":{}}]}